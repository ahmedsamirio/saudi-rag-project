{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install firectl on Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O firectl.gz https://storage.googleapis.com/fireworks-public/firectl/stable/linux-amd64.gz\n",
    "!gunzip firectl.gz\n",
    "!sudo install -o root -g root -m 0755 firectl /usr/local/bin/firectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"malhajar/alpaca-gpt4-ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x['input'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20658"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_PROMPT_TEMPLATE_AR = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant. Answer in Arabic only. \\\n",
    "{instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "LLAMA_PROMPT_TEMPLATE_EN = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant. \\\n",
    "{instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = \"<|eot_id|>\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions_ar = examples[\"instruction-arabic\"]\n",
    "    inputs_ar       = examples[\"input-arabic\"]\n",
    "\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "\n",
    "    en_instruction_en_input_en_prompts = []\n",
    "    en_instruction_ar_input_ar_prompts = []\n",
    "    en_instruction_en_input_ar_prompts = []\n",
    "\n",
    "    for instruction_, input_ in zip(instructions, inputs):\n",
    "        en_instruction_en_input_en_output = LLAMA_PROMPT_TEMPLATE_EN.format(instruction=instruction_, input=input_) + EOS_TOKEN\n",
    "        en_instruction_en_input_en_prompts.append(en_instruction_en_input_en_output)\n",
    "\n",
    "    for instruction_, input_ in zip(instructions, inputs_ar):\n",
    "        en_instruction_ar_input_ar_output = LLAMA_PROMPT_TEMPLATE_AR.format(instruction=instruction_, input=input_) + EOS_TOKEN\n",
    "        en_instruction_ar_input_ar_prompts.append(en_instruction_ar_input_ar_output)\n",
    "\n",
    "    for instruction_, input_ in zip(instructions, inputs):\n",
    "        en_instruction_en_input_ar_output = LLAMA_PROMPT_TEMPLATE_AR.format(instruction=instruction_, input=input_) + EOS_TOKEN\n",
    "        en_instruction_en_input_ar_prompts.append(en_instruction_en_input_ar_output)\n",
    "\n",
    "    return {\n",
    "        \"en_instruction_en_input_en_prompts\" : en_instruction_en_input_en_prompts, \n",
    "        \"en_instruction_ar_input_ar_prompts\": en_instruction_ar_input_ar_prompts,\n",
    "        \"en_instruction_en_input_ar_prompts\": en_instruction_en_input_ar_prompts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061feb5a9c1b4dfcae389b64ab409c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the samples having the highest input size, which is similar to the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20658/20658 [00:07<00:00, 2619.82it/s]\n"
     ]
    }
   ],
   "source": [
    "input_length = []\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "\n",
    "    input_length.append(len(sample['input']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(input_length)[-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I'll select the top 5000 and test my luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text', 'instruction-arabic', 'input-arabic', 'output-arabic', 'text-arabic', 'en_instruction_en_input_en_prompts', 'en_instruction_ar_input_ar_prompts', 'en_instruction_en_input_ar_prompts'],\n",
       "        num_rows: 20658\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee7891939bf484091da6d110a2b5399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_dataset = dataset.filter(lambda x: len(x['input']) >= 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5096"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15288 lines converted\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "split_data = final_dataset[\"train\"]\n",
    "\n",
    "counter = 0\n",
    "with open(\"llama3-8b-alpaca-ar.jsonl\", \"w\") as f:\n",
    "    for item in split_data:\n",
    "\n",
    "        en_instruction_en_input_en_outout_prompt = {\"input\": item[\"en_instruction_en_input_en_prompts\"], \"output\": item[\"output\"]}\n",
    "        en_instruction_ar_input_ar_output_prompt = {\"input\": item[\"en_instruction_ar_input_ar_prompts\"], \"output\": item[\"output-arabic\"]}\n",
    "        en_instruction_en_input_ar_output_prompt = {\"input\": item[\"en_instruction_en_input_ar_prompts\"], \"output\": item[\"output-arabic\"]}\n",
    "        \n",
    "        json.dump(en_instruction_en_input_en_outout_prompt, f)\n",
    "        counter += f.write(\"\\n\")\n",
    "\n",
    "        json.dump(en_instruction_ar_input_ar_output_prompt, f)\n",
    "        counter += f.write(\"\\n\")\n",
    "\n",
    "        json.dump(en_instruction_en_input_ar_output_prompt, f)\n",
    "        counter += f.write(\"\\n\")\n",
    "\n",
    "print(f\"{counter} lines converted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start finetuneing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!firectl create fine-tuning-job --settings-file llama3-8b-alpaca-ar.jsonl --display-name \"Llama3 8b Alpaca Ar Finetune\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saudi-rag-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
