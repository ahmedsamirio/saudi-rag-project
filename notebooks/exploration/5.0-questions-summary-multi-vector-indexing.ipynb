{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.indexing import get_multivector_retriever, get_parent_child_splits\n",
    "from src.generation import QA_SYSTEM_PROMPT, QA_PROMPT, LLAMA_PROMPT_TEMPLATE, MIXTRAL_PROMPT_TEMPLATE\n",
    "from src.generation import get_model, format_docs, get_rag_chain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from src.ingestion import load_pdf\n",
    "\n",
    "import os\n",
    "import chromadb\n",
    "import uuid\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_19012\\1562872242.py:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  DATA_PATH = 'D:\\Ahmed\\saudi-rag-project\\data'\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'D:\\Ahmed\\saudi-rag-project\\data'\n",
    "RAW_DOCS_PATH = os.path.join(DATA_PATH, \"raw\")\n",
    "CHROMA_PATH = os.path.join(DATA_PATH, \"chroma\")\n",
    "INTERIM_DATA_PATH = os.path.join(DATA_PATH, \"interim\")\n",
    "\n",
    "EMBEDDING_MODEL_NAMES = [\n",
    "    \"intfloat/multilingual-e5-small\", \n",
    "    \"intfloat/multilingual-e5-base\", \n",
    "    \"text-embedding-3-small\", \n",
    "    \"text-embedding-3-large\",\n",
    "    \"text-embedding-ada-002\"\n",
    " ]\n",
    "MODEL_NAMES = [\"meta-llama/Llama-3-8b-chat-hf\", \"meta-llama/Llama-3-70b-chat-hf\", \"mistralai/Mixtral-8x22B-Instruct-v0.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [load_pdf(os.path.join(RAW_DOCS_PATH, f)) for f in os.listdir(RAW_DOCS_PATH) if \".pdf\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=400,\n",
    "    separators=['\\n\\n\\n', '\\n\\n', '\\n', r'\\.\\s+', ' ', '']\n",
    ")\n",
    "\n",
    "parent_docs = parent_splitter.split_documents(docs)\n",
    "# parent_docs_ids = [str(uuid.uuid4()) for _ in parent_docs]\n",
    "# pickle.dump(parent_docs_ids, open(os.path.join(INTERIM_DATA_PATH, \"parent_docs_ids\"), 'wb'))\n",
    "parent_docs_ids = pickle.load(open(os.path.join(INTERIM_DATA_PATH, \"parent_docs_ids\"), 'rb'))\n",
    "id_key = \"parent_doc_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_docs_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create regular child splitters with each embedding model with different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parent_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:24<00:00, 28.81s/it]\n"
     ]
    }
   ],
   "source": [
    "for embedding_model_name in tqdm(EMBEDDING_MODEL_NAMES):\n",
    "    for child_chunk_size in [500, 300, 100]:\n",
    "        collection_name = f\"PC_{child_chunk_size}_{embedding_model_name.split('/')[-1].replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "\n",
    "        child_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=child_chunk_size,\n",
    "            chunk_overlap=0,\n",
    "        )\n",
    "\n",
    "        child_docs = []\n",
    "        for i, doc in enumerate(parent_docs):\n",
    "            _id = parent_docs_ids[i]\n",
    "            _child_docs = child_splitter.split_documents([doc])\n",
    "            for _doc in _child_docs:\n",
    "                _doc.metadata[id_key] = _id\n",
    "            child_docs.extend(_child_docs)\n",
    "\n",
    "        _ = get_multivector_retriever(persistent_client, embedding_model_name, collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=child_docs, id_key=\"parent_doc_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create retrievers with different child chunks sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [13:24<00:00, 160.99s/it]\n"
     ]
    }
   ],
   "source": [
    "for embedding_model_name in tqdm(EMBEDDING_MODEL_NAMES):\n",
    "    for child_chunk_sizes in [(500, 100), (500, 300), (300, 100), (500, 300, 100)]:\n",
    "        all_child_docs = []\n",
    "        collection_name = f\"PC_{'_'.join([str(x) for x in child_chunk_sizes])}_{embedding_model_name.split('/')[-1].replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "\n",
    "        for child_chunk_size in child_chunk_sizes:\n",
    "            child_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=child_chunk_size,\n",
    "                chunk_overlap=0,\n",
    "            )\n",
    "\n",
    "            child_docs = []\n",
    "            for i, doc in enumerate(parent_docs):\n",
    "                _id = parent_docs_ids[i]\n",
    "                _child_docs = child_splitter.split_documents([doc])\n",
    "                for _doc in _child_docs:\n",
    "                    _doc.metadata[id_key] = _id\n",
    "                child_docs.extend(_child_docs)\n",
    "\n",
    "            all_child_docs.extend(child_docs)\n",
    "\n",
    "        _ = get_multivector_retriever(persistent_client, embedding_model_name, collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_child_docs, id_key=\"parent_doc_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate questions for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS_SYSTEM_PROMPT = \"\"\"Write a list of 30 fact based simple questions in Arabic that can be answered using the document. \\\n",
    "The questions should be require specific numbers or information mentioned in the document for an answer. \\\n",
    "But the questions should include information present in the document itself. \\\n",
    "Each question should be understandable indepdent from any context. \\\n",
    "You SHOULD NOT use any numbers in the question (except for years). \\\n",
    "Write the questions in Arabic only and don't write the answers. \\\n",
    "Output the questions directly without an introduction.\"\"\"\n",
    "\n",
    "SUMMARY_SYSTEM_PROMPT = \"\"\"You are an expert summary writer. \\\n",
    "Write a title and summarize the provided context in an entity dense way. \\\n",
    "The summary and title should be in Arabic only.\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"\"\"Context: {context}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate questions from all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "def is_english(text):\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang == 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistralai/Mixtral-8x22B-Instruct-v0.1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAMES[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [08:03<00:00, 483.14s/it]\n"
     ]
    }
   ],
   "source": [
    "for model_name in tqdm(MODEL_NAMES[1:2]):\n",
    "\n",
    "    if \"llama\" in model_name:\n",
    "        prompt_template = PromptTemplate.from_template(LLAMA_PROMPT_TEMPLATE.format(system_prompt=QUESTIONS_SYSTEM_PROMPT, user_message=USER_MESSAGE))\n",
    "        llm = get_model(model_name)\n",
    "\n",
    "    if \"mistral\" in model_name:\n",
    "        prompt_template = PromptTemplate.from_template(MIXTRAL_PROMPT_TEMPLATE.format(system_prompt=QUESTIONS_SYSTEM_PROMPT, user_message=USER_MESSAGE))\n",
    "        llm = get_model(model_name, max_tokens=2048)\n",
    "\n",
    "    question_chain = (\n",
    "        prompt_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    _question_docs = question_chain.batch(parent_docs)\n",
    "\n",
    "    question_docs = []\n",
    "    split_question_docs = []\n",
    "    for i, doc in enumerate(_question_docs):\n",
    "        _id = parent_docs_ids[i]\n",
    "        _doc = Document(page_content=doc, metadata={id_key: _id, 'source': parent_docs[i].metadata['source']})\n",
    "        question_docs.append(_doc)\n",
    "        for l in doc.split('\\n'):\n",
    "            if not is_english(l): \n",
    "                _question = Document(page_content=l, metadata={id_key: _id, 'source': parent_docs[i].metadata['source']})\n",
    "                split_question_docs.append(_question)\n",
    "        \n",
    "\n",
    "    question_file = f\"PQ_COMB_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    question_file_path = os.path.join(INTERIM_DATA_PATH, question_file)\n",
    "\n",
    "    split_question_file = f\"PQ_SPLIT_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    split_question_file_path = os.path.join(INTERIM_DATA_PATH, split_question_file)\n",
    "\n",
    "    pickle.dump(question_docs, open(question_file_path, 'wb'))\n",
    "    pickle.dump(split_question_docs, open(split_question_file_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate summaries from all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [11:28<00:00, 688.96s/it]\n"
     ]
    }
   ],
   "source": [
    "for model_name in tqdm(MODEL_NAMES[2:]):\n",
    "\n",
    "    if \"llama\" in model_name:\n",
    "        prompt_template = PromptTemplate.from_template(LLAMA_PROMPT_TEMPLATE.format(system_prompt=SUMMARY_SYSTEM_PROMPT, user_message=USER_MESSAGE))\n",
    "    if \"mistral\" in model_name:\n",
    "        prompt_template = PromptTemplate.from_template(MIXTRAL_PROMPT_TEMPLATE.format(system_prompt=SUMMARY_SYSTEM_PROMPT, user_message=USER_MESSAGE))\n",
    "\n",
    "    llm = get_model(model_name)\n",
    "\n",
    "    summary_chain = (\n",
    "        prompt_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    _summary_docs = summary_chain.batch(parent_docs)\n",
    "\n",
    "    summary_docs = []\n",
    "    for i, doc in enumerate(_summary_docs):\n",
    "        _id = parent_docs_ids[i]\n",
    "        _doc = Document(page_content=doc, metadata={id_key: _id, 'source': parent_docs[i].metadata['source']})\n",
    "        summary_docs.append(_doc)\n",
    "        \n",
    "\n",
    "    summary_file = f\"PS_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    summary_file_path = os.path.join(INTERIM_DATA_PATH, summary_file)\n",
    "\n",
    "    pickle.dump(summary_docs, open(summary_file_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PQ_COMB_Llama_3_8b_chat_hf_intfloat_multilingual_e5_small\n",
      "Creating PQ_SPLIT_Llama_3_8b_chat_hf_intfloat_multilingual_e5_small\n",
      "Creating PS_Llama_3_8b_chat_hf_intfloat_multilingual_e5_small\n",
      "Creating PQ_COMB_Llama_3_8b_chat_hf_intfloat_multilingual_e5_base\n",
      "Creating PQ_SPLIT_Llama_3_8b_chat_hf_intfloat_multilingual_e5_base\n",
      "Creating PS_Llama_3_8b_chat_hf_intfloat_multilingual_e5_base\n",
      "Creating PQ_COMB_Llama_3_8b_chat_hf_text_embedding_3_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\miniconda3\\envs\\saudi-rag-project\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PQ_SPLIT_Llama_3_8b_chat_hf_text_embedding_3_small\n",
      "Creating PS_Llama_3_8b_chat_hf_text_embedding_3_small\n",
      "Creating PQ_COMB_Llama_3_8b_chat_hf_text_embedding_3_large\n",
      "Creating PQ_SPLIT_Llama_3_8b_chat_hf_text_embedding_3_large\n",
      "Creating PS_Llama_3_8b_chat_hf_text_embedding_3_large\n",
      "Creating PQ_COMB_Llama_3_8b_chat_hf_text_embedding_ada_002\n",
      "Creating PQ_SPLIT_Llama_3_8b_chat_hf_text_embedding_ada_002\n",
      "Creating PS_Llama_3_8b_chat_hf_text_embedding_ada_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [16:22<32:44, 982.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PQ_COMB_Llama_3_70b_chat_hf_intfloat_multilingual_e5_small\n",
      "Creating PQ_SPLIT_Llama_3_70b_chat_hf_intfloat_multilingual_e5_small\n",
      "Creating PS_Llama_3_70b_chat_hf_intfloat_multilingual_e5_small\n",
      "Creating PQ_COMB_Llama_3_70b_chat_hf_intfloat_multilingual_e5_base\n",
      "Creating PQ_SPLIT_Llama_3_70b_chat_hf_intfloat_multilingual_e5_base\n",
      "Creating PS_Llama_3_70b_chat_hf_intfloat_multilingual_e5_base\n",
      "Creating PQ_COMB_Llama_3_70b_chat_hf_text_embedding_3_small\n",
      "Creating PQ_SPLIT_Llama_3_70b_chat_hf_text_embedding_3_small\n",
      "Creating PS_Llama_3_70b_chat_hf_text_embedding_3_small\n",
      "Creating PQ_COMB_Llama_3_70b_chat_hf_text_embedding_3_large\n",
      "Creating PQ_SPLIT_Llama_3_70b_chat_hf_text_embedding_3_large\n",
      "Creating PS_Llama_3_70b_chat_hf_text_embedding_3_large\n",
      "Creating PQ_COMB_Llama_3_70b_chat_hf_text_embedding_ada_002\n",
      "Creating PQ_SPLIT_Llama_3_70b_chat_hf_text_embedding_ada_002\n",
      "Creating PS_Llama_3_70b_chat_hf_text_embedding_ada_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [24:08<11:18, 678.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PQ_COMB_Mixtral_8x22B_intfloat_multilingual_e5_small\n",
      "Creating PQ_SPLIT_Mixtral_8x22B_intfloat_multilingual_e5_small\n",
      "Creating PS_Mixtral_8x22B_intfloat_multilingual_e5_small\n",
      "Creating PQ_COMB_Mixtral_8x22B_intfloat_multilingual_e5_base\n",
      "Creating PQ_SPLIT_Mixtral_8x22B_intfloat_multilingual_e5_base\n",
      "Creating PS_Mixtral_8x22B_intfloat_multilingual_e5_base\n",
      "Creating PQ_COMB_Mixtral_8x22B_text_embedding_3_small\n",
      "Creating PQ_SPLIT_Mixtral_8x22B_text_embedding_3_small\n",
      "Creating PS_Mixtral_8x22B_text_embedding_3_small\n",
      "Creating PQ_COMB_Mixtral_8x22B_text_embedding_3_large\n",
      "Creating PQ_SPLIT_Mixtral_8x22B_text_embedding_3_large\n",
      "Creating PS_Mixtral_8x22B_text_embedding_3_large\n",
      "Creating PQ_COMB_Mixtral_8x22B_text_embedding_ada_002\n",
      "Creating PQ_SPLIT_Mixtral_8x22B_text_embedding_ada_002\n",
      "Creating PS_Mixtral_8x22B_text_embedding_ada_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [24:11<00:00, 483.75s/it]\n"
     ]
    }
   ],
   "source": [
    "all_questions_docs = []\n",
    "all_split_questions_docs = []\n",
    "all_summaries_docs = []\n",
    "\n",
    "for model_name in tqdm(MODEL_NAMES):\n",
    "\n",
    "    question_file = f\"PQ_COMB_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    question_file_path = os.path.join(INTERIM_DATA_PATH, question_file)\n",
    "\n",
    "    split_question_file = f\"PQ_SPLIT_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    split_question_file_path = os.path.join(INTERIM_DATA_PATH, split_question_file)\n",
    "\n",
    "    summary_file = f\"PS_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    summary_file_path = os.path.join(INTERIM_DATA_PATH, summary_file)\n",
    "\n",
    "    question_docs = pickle.load(open(question_file_path, 'rb'))\n",
    "    split_question_docs = pickle.load(open(split_question_file_path, 'rb'))\n",
    "    summary_docs = pickle.load(open(summary_file_path, 'rb'))\n",
    "\n",
    "    all_questions_docs.extend(question_docs)\n",
    "    all_split_questions_docs.extend(split_question_docs)\n",
    "    all_summaries_docs.extend(summary_docs)\n",
    "\n",
    "    for embedding_model_name in EMBEDDING_MODEL_NAMES:\n",
    "\n",
    "        if \"mistral\" in model_name:\n",
    "            model_name = model_name.split('/')[-1].replace('-', '_').replace('/', '_')[:13]\n",
    "        else:\n",
    "            model_name = model_name.split('/')[-1].replace('-', '_').replace('/', '_')\n",
    "\n",
    "        questions_collection_name = f\"PQ_COMB_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "        split_questions_collection_name = f\"PQ_SPLIT_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "        summary_collection_name = f\"PS_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "\n",
    "        if questions_collection_name not in persistent_client.list_collections():\n",
    "            print(\"Creating\", questions_collection_name)\n",
    "            _ = get_multivector_retriever(persistent_client, embedding_model_name, questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=question_docs, id_key=\"parent_doc_id\")\n",
    "        \n",
    "        if split_questions_collection_name not in persistent_client.list_collections():\n",
    "            print(\"Creating\", split_questions_collection_name)\n",
    "            _ = get_multivector_retriever(persistent_client, embedding_model_name, split_questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=split_question_docs, id_key=\"parent_doc_id\")\n",
    "        \n",
    "        if summary_collection_name not in persistent_client.list_collections():\n",
    "            print(\"Creating\", summary_collection_name)\n",
    "            _ = get_multivector_retriever(persistent_client, embedding_model_name, summary_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=summary_docs, id_key=\"parent_doc_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create one vectorstore with all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [17:03<1:08:15, 1023.85s/it]"
     ]
    }
   ],
   "source": [
    "for embedding_model_name in tqdm(EMBEDDING_MODEL_NAMES):\n",
    "\n",
    "    all_questions_collection_name = f\"PQ_COMB_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    all_split_questions_collection_name = f\"PQ_SPLIT_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    all_summary_collection_name = f\"PS_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    all_collection_name = f\"PQS_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "\n",
    "    if all_questions_collection_name in persistent_client.list_collections():\n",
    "        persistent_client.delete_collection(all_questions_collection_name)\n",
    "\n",
    "    if all_split_questions_collection_name in persistent_client.list_collections():\n",
    "        persistent_client.delete_collection(all_split_questions_collection_name)\n",
    "\n",
    "    if all_summary_collection_name in persistent_client.list_collections():\n",
    "        persistent_client.delete_collection(all_summary_collection_name)\n",
    "\n",
    "    if all_collection_name in persistent_client.list_collections():\n",
    "        persistent_client.delete_collection(all_collection_name)\n",
    "\n",
    "    all_generated_docs = all_questions_docs + all_split_questions_docs + all_summaries_docs\n",
    "\n",
    "    _ = get_multivector_retriever(persistent_client, embedding_model_name, all_questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_questions_docs, id_key=\"parent_doc_id\")\n",
    "    _ = get_multivector_retriever(persistent_client, embedding_model_name, all_split_questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_split_questions_docs, id_key=\"parent_doc_id\")\n",
    "    _ = get_multivector_retriever(persistent_client, embedding_model_name, all_summary_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_summaries_docs, id_key=\"parent_doc_id\")\n",
    "    _ = get_multivector_retriever(persistent_client, embedding_model_name, all_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_generated_docs, id_key=\"parent_doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3-8b-chat-hf\n",
      "48\n",
      "48\n",
      "48\n",
      "meta-llama/Llama-3-70b-chat-hf\n",
      "48\n",
      "48\n",
      "48\n",
      "mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "48\n",
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "for model_name in MODEL_NAMES:\n",
    "    print(model_name)\n",
    "    question_file = f\"PQ_COMB_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    question_file_path = os.path.join(INTERIM_DATA_PATH, question_file)\n",
    "\n",
    "    split_question_file = f\"PQ_SPLIT_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    split_question_file_path = os.path.join(INTERIM_DATA_PATH, split_question_file)\n",
    "\n",
    "    summary_file = f\"PS_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    summary_file_path = os.path.join(INTERIM_DATA_PATH, summary_file)\n",
    "\n",
    "    question_docs = pickle.load(open(question_file_path, 'rb'))\n",
    "    split_question_docs = pickle.load(open(split_question_file_path, 'rb'))\n",
    "    summary_docs = pickle.load(open(summary_file_path, 'rb'))\n",
    "\n",
    "    _ids = [d.metadata['parent_doc_id'] for d in question_docs]\n",
    "    print(len(set(parent_docs_ids).intersection(_ids)))\n",
    "\n",
    "    _ids = [d.metadata['parent_doc_id'] for d in split_question_docs]\n",
    "    print(len(set(parent_docs_ids).intersection(_ids)))\n",
    "\n",
    "    _ids = [d.metadata['parent_doc_id'] for d in summary_docs]\n",
    "    print(len(set(parent_docs_ids).intersection(_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    " _ids = [d.metadata['parent_doc_id'] for d in all_questions_docs]\n",
    "print(len(set(parent_docs_ids).intersection(_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    " _ids = [d.metadata['parent_doc_id'] for d in all_split_questions_docs]\n",
    "print(len(set(parent_docs_ids).intersection(_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    " _ids = [d.metadata['parent_doc_id'] for d in all_summaries_docs]\n",
    "print(len(set(parent_docs_ids).intersection(_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saudi-rag-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
