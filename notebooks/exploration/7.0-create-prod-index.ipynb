{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_20028\\3267159222.py:19: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  DATA_PATH = 'D:\\Ahmed\\saudi-rag-project\\storage'\n"
     ]
    }
   ],
   "source": [
    "from src.indexing import get_multivector_retriever, get_parent_child_splits\n",
    "from src.generation import QA_SYSTEM_PROMPT, QA_PROMPT, LLAMA_PROMPT_TEMPLATE, MIXTRAL_PROMPT_TEMPLATE\n",
    "from src.generation import get_model, format_docs, get_rag_chain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from src.ingestion import load_pdf\n",
    "\n",
    "import os\n",
    "import chromadb\n",
    "import uuid\n",
    "import pickle\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH = 'D:\\Ahmed\\saudi-rag-project\\storage'\n",
    "RAW_DOCS_PATH = os.path.join('../../data', \"raw\")\n",
    "CHROMA_PATH = os.path.join(DATA_PATH, \"chroma\")\n",
    "INTERIM_DATA_PATH = os.path.join('../../data', \"interim\")\n",
    "\n",
    "EMBEDDING_MODEL_NAMES = [\n",
    "    # \"intfloat/multilingual-e5-small\", \n",
    "    # \"intfloat/multilingual-e5-base\", \n",
    "    # \"text-embedding-3-small\", \n",
    "    \"text-embedding-3-large\",\n",
    "    # \"text-embedding-ada-002\"\n",
    " ]\n",
    " \n",
    "MODEL_NAMES = [\"meta-llama/Llama-3-8b-chat-hf\", \"meta-llama/Llama-3-70b-chat-hf\", \"mistralai/Mixtral-8x22B-Instruct-v0.1\"]\n",
    "\n",
    "COLLECTIONS = [\"PQ_COMB_Llama_3_70b_chat_hf_text_embedding_3_large\", \"PQ_COMB_S_Llama_3_70b_chat_hf_text_embedding_3_large\"]\n",
    "\n",
    "docs = [load_pdf(os.path.join(RAW_DOCS_PATH, f)) for f in os.listdir(RAW_DOCS_PATH) if \".pdf\" in f]\n",
    "persistent_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=400,\n",
    "    separators=['\\n\\n\\n', '\\n\\n', '\\n', r'\\.\\s+', ' ', '']\n",
    ")\n",
    "\n",
    "parent_docs = parent_splitter.split_documents(docs)\n",
    "# parent_docs_ids = [str(uuid.uuid4()) for _ in parent_docs]\n",
    "# pickle.dump(parent_docs_ids, open(os.path.join(INTERIM_DATA_PATH, \"parent_docs_ids\"), 'wb'))\n",
    "parent_docs_ids = pickle.load(open(os.path.join(INTERIM_DATA_PATH, \"parent_docs_ids\"), 'rb'))\n",
    "id_key = \"parent_doc_id\"\n",
    "\n",
    "all_questions_docs = []\n",
    "all_split_questions_docs = []\n",
    "all_summaries_docs = []\n",
    "\n",
    "for model_name in tqdm(MODEL_NAMES):\n",
    "\n",
    "    question_file = f\"PQ_COMB_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    question_file_path = os.path.join(INTERIM_DATA_PATH, question_file)\n",
    "\n",
    "    split_question_file = f\"PQ_SPLIT_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    split_question_file_path = os.path.join(INTERIM_DATA_PATH, split_question_file)\n",
    "\n",
    "    summary_file = f\"PS_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    summary_file_path = os.path.join(INTERIM_DATA_PATH, summary_file)\n",
    "\n",
    "    question_docs = pickle.load(open(question_file_path, 'rb'))\n",
    "    split_question_docs = pickle.load(open(split_question_file_path, 'rb'))\n",
    "    summary_docs = pickle.load(open(summary_file_path, 'rb'))\n",
    "\n",
    "    all_questions_docs.extend(question_docs)\n",
    "    all_split_questions_docs.extend(split_question_docs)\n",
    "    all_summaries_docs.extend(summary_docs)\n",
    "\n",
    "    all_questions_summaries_docs = all_questions_docs + all_summaries_docs\n",
    "\n",
    "    for embedding_model_name in EMBEDDING_MODEL_NAMES:\n",
    "\n",
    "        if \"mistral\" in model_name:\n",
    "            model_name = model_name.split('/')[-1].replace('-', '_').replace('/', '_')[:13]\n",
    "        else:\n",
    "            model_name = model_name.split('/')[-1].replace('-', '_').replace('/', '_')\n",
    "\n",
    "        questions_collection_name = f\"PQ_COMB_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "        split_questions_collection_name = f\"PQ_SPLIT_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "        summary_collection_name = f\"PS_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "        questions_summary_collection_name = f\"PQ_COMB_S_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "\n",
    "        if questions_collection_name in COLLECTIONS:\n",
    "            print(\"Creating\", questions_collection_name)\n",
    "            _ = get_multivector_retriever(persistent_client, embedding_model_name, questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=question_docs, id_key=\"parent_doc_id\")\n",
    "        \n",
    "        if split_questions_collection_name in COLLECTIONS:\n",
    "            print(\"Creating\", split_questions_collection_name)\n",
    "            _ = get_multivector_retriever(persistent_client, embedding_model_name, split_questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=split_question_docs, id_key=\"parent_doc_id\")\n",
    "        \n",
    "        if summary_collection_name in COLLECTIONS:\n",
    "            print(\"Creating\", summary_collection_name)\n",
    "            _ = get_multivector_retriever(persistent_client, embedding_model_name, summary_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_questions_summaries_docs, id_key=\"parent_doc_id\")\n",
    "\n",
    "        if questions_summary_collection_name in COLLECTIONS:\n",
    "            print(\"Creating\", summary_collection_name)\n",
    "            _ = get_multivector_retriever(persistent_client, embedding_model_name, questions_summary_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_questions_summaries_docs, id_key=\"parent_doc_id\")\n",
    "\n",
    "    x = []\n",
    "\n",
    "for embedding_model_name in tqdm(EMBEDDING_MODEL_NAMES):\n",
    "\n",
    "    print(embedding_model_name)\n",
    "\n",
    "    all_questions_collection_name = f\"PQ_COMB_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    all_split_questions_collection_name = f\"PQ_SPLIT_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    all_summary_collection_name = f\"PS_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    all_collection_name = f\"PQS_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "\n",
    "    # if all_questions_collection_name in persistent_client.list_collections():\n",
    "    #     persistent_client.delete_collection(all_questions_collection_name)\n",
    "\n",
    "    # if all_split_questions_collection_name in persistent_client.list_collections():\n",
    "    #     persistent_client.delete_collection(all_split_questions_collection_name)\n",
    "\n",
    "    # if all_summary_collection_name in persistent_client.list_collections():\n",
    "    #     persistent_client.delete_collection(all_summary_collection_name)\n",
    "\n",
    "    # if all_collection_name in persistent_client.list_collections():\n",
    "    #     persistent_client.delete_collection(all_collection_name)\n",
    "\n",
    "    # all_generated_docs = all_questions_docs + all_split_questions_docs + all_summaries_docs\n",
    "\n",
    "    # print(all_questions_collection_name)\n",
    "    # r = get_multivector_retriever(persistent_client, embedding_model_name, all_questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_questions_docs, id_key=\"parent_doc_id\")\n",
    "    # x.append(r)\n",
    "\n",
    "    # if all_split_questions_collection_name in COLLECTIONS: \n",
    "    # print(all_split_questions_collection_name)\n",
    "    # r = get_multivector_retriever(persistent_client, embedding_model_name, all_split_questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_split_questions_docs, id_key=\"parent_doc_id\")\n",
    "    # x.append(r)\n",
    "    # print(all_summary_collection_name)\n",
    "    # r = get_multivector_retriever(persistent_client, embedding_model_name, all_summary_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_summaries_docs, id_key=\"parent_doc_id\")\n",
    "    # x.append(r)\n",
    "\n",
    "    # if all_collection_name in COLLECTIONS:\n",
    "    #     _ = get_multivector_retriever(persistent_client, embedding_model_name, all_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_generated_docs, id_key=\"parent_doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10888\\3734333639.py:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  DATA_PATH = 'D:\\Ahmed\\saudi-rag-project\\data\\storage'\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10888\\3734333639.py:2: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  RAW_DOCS_PATH = os.path.join('..\\..\\data', \"raw\")\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_10888\\3734333639.py:4: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  INTERIM_DATA_PATH = os.path.join('..\\..\\data', \"interim\")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_client.delete_collection('PS_ALL_text_embedding_3_small')\n",
    "persistent_client.delete_collection('PQ_SPLIT_ALL_text_embedding_3_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=400,\n",
    "    separators=['\\n\\n\\n', '\\n\\n', '\\n', r'\\.\\s+', ' ', '']\n",
    ")\n",
    "\n",
    "parent_docs = parent_splitter.split_documents(docs)\n",
    "# parent_docs_ids = [str(uuid.uuid4()) for _ in parent_docs]\n",
    "# pickle.dump(parent_docs_ids, open(os.path.join(INTERIM_DATA_PATH, \"parent_docs_ids\"), 'wb'))\n",
    "parent_docs_ids = pickle.load(open(os.path.join(INTERIM_DATA_PATH, \"parent_docs_ids\"), 'rb'))\n",
    "id_key = \"parent_doc_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTIONS = [\n",
    "    \"PQ_SPLIT_ALL_text_embedding_3_small\",\n",
    "    \"PS_ALL_text_embedding_3_small\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 85.95it/s]\n"
     ]
    }
   ],
   "source": [
    "all_questions_docs = []\n",
    "all_split_questions_docs = []\n",
    "all_summaries_docs = []\n",
    "\n",
    "for model_name in tqdm(MODEL_NAMES):\n",
    "\n",
    "    question_file = f\"PQ_COMB_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    question_file_path = os.path.join(INTERIM_DATA_PATH, question_file)\n",
    "\n",
    "    split_question_file = f\"PQ_SPLIT_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    split_question_file_path = os.path.join(INTERIM_DATA_PATH, split_question_file)\n",
    "\n",
    "    summary_file = f\"PS_{model_name.split('/')[-1].replace('-', '_').replace('/', '_')}\"\n",
    "    summary_file_path = os.path.join(INTERIM_DATA_PATH, summary_file)\n",
    "\n",
    "    question_docs = pickle.load(open(question_file_path, 'rb'))\n",
    "    split_question_docs = pickle.load(open(split_question_file_path, 'rb'))\n",
    "    summary_docs = pickle.load(open(summary_file_path, 'rb'))\n",
    "\n",
    "    all_questions_docs.extend(question_docs)\n",
    "    all_split_questions_docs.extend(split_question_docs)\n",
    "    all_summaries_docs.extend(summary_docs)\n",
    "\n",
    "    # for embedding_model_name in EMBEDDING_MODEL_NAMES:\n",
    "\n",
    "    #     if \"mistral\" in model_name:\n",
    "    #         model_name = model_name.split('/')[-1].replace('-', '_').replace('/', '_')[:13]\n",
    "    #     else:\n",
    "    #         model_name = model_name.split('/')[-1].replace('-', '_').replace('/', '_')\n",
    "\n",
    "    #     questions_collection_name = f\"PQ_COMB_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    #     split_questions_collection_name = f\"PQ_SPLIT_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    #     summary_collection_name = f\"PS_{model_name}_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "\n",
    "    #     if questions_collection_name not in persistent_client.list_collections():\n",
    "    #         print(\"Creating\", questions_collection_name)\n",
    "    #         _ = get_multivector_retriever(persistent_client, embedding_model_name, questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=question_docs, id_key=\"parent_doc_id\")\n",
    "        \n",
    "    #     if split_questions_collection_name not in persistent_client.list_collections():\n",
    "    #         print(\"Creating\", split_questions_collection_name)\n",
    "    #         _ = get_multivector_retriever(persistent_client, embedding_model_name, split_questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=split_question_docs, id_key=\"parent_doc_id\")\n",
    "        \n",
    "    #     if summary_collection_name not in persistent_client.list_collections():\n",
    "    #         print(\"Creating\", summary_collection_name)\n",
    "    #         _ = get_multivector_retriever(persistent_client, embedding_model_name, summary_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=summary_docs, id_key=\"parent_doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('../../storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-embedding-3-small\n",
      "PQ_SPLIT_ALL_text_embedding_3_small\n",
      "../storage\\docstore\\PQ_SPLIT_ALL_text_embedding_3_small\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 32\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# if all_questions_collection_name in persistent_client.list_collections():\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#     persistent_client.delete_collection(all_questions_collection_name)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# if all_split_questions_collection_name in COLLECTIONS: \u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_split_questions_collection_name)\n\u001b[1;32m---> 32\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mget_multivector_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersistent_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_split_questions_collection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_docs_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_docs_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_split_questions_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparent_doc_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m x\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_summary_collection_name)\n",
      "File \u001b[1;32md:\\Ahmed\\saudi-rag-project\\notebooks\\exploration\\../..\\src\\indexing.py:63\u001b[0m, in \u001b[0;36mget_multivector_retriever\u001b[1;34m(chroma_client, embedding_model_name, collection_name, save_path, parent_docs, parent_docs_ids, child_docs, id_key, k)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(docstore_path))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Get embedding_function\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m embedding_function \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma(\n\u001b[0;32m     66\u001b[0m     client\u001b[38;5;241m=\u001b[39mchroma_client,\n\u001b[0;32m     67\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m     68\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[0;32m     69\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mvectorstore_path\n\u001b[0;32m     70\u001b[0m )\n\u001b[0;32m     72\u001b[0m store \u001b[38;5;241m=\u001b[39m LocalFileStore(docstore_path)\n",
      "File \u001b[1;32md:\\Ahmed\\saudi-rag-project\\notebooks\\exploration\\../..\\src\\indexing.py:48\u001b[0m, in \u001b[0;36mget_embedding_function\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     45\u001b[0m load_dotenv(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../.env\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\saudi-rag-project\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     emit_warning()\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\miniconda3\\envs\\saudi-rag-project\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "\n",
    "for embedding_model_name in tqdm(EMBEDDING_MODEL_NAMES):\n",
    "\n",
    "    print(embedding_model_name)\n",
    "\n",
    "    all_questions_collection_name = f\"PQ_COMB_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    all_split_questions_collection_name = f\"PQ_SPLIT_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    all_summary_collection_name = f\"PS_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "    all_collection_name = f\"PQS_ALL_{embedding_model_name.replace(\"-\", \"_\").replace(\"/\", \"_\")}\"\n",
    "\n",
    "    # if all_questions_collection_name in persistent_client.list_collections():\n",
    "    #     persistent_client.delete_collection(all_questions_collection_name)\n",
    "\n",
    "    # if all_split_questions_collection_name in persistent_client.list_collections():\n",
    "    #     persistent_client.delete_collection(all_split_questions_collection_name)\n",
    "\n",
    "    # if all_summary_collection_name in persistent_client.list_collections():\n",
    "    #     persistent_client.delete_collection(all_summary_collection_name)\n",
    "\n",
    "    # if all_collection_name in persistent_client.list_collections():\n",
    "    #     persistent_client.delete_collection(all_collection_name)\n",
    "\n",
    "    # all_generated_docs = all_questions_docs + all_split_questions_docs + all_summaries_docs\n",
    "\n",
    "    # print(all_questions_collection_name)\n",
    "    # r = get_multivector_retriever(persistent_client, embedding_model_name, all_questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_questions_docs, id_key=\"parent_doc_id\")\n",
    "    # x.append(r)\n",
    "\n",
    "    # if all_split_questions_collection_name in COLLECTIONS: \n",
    "    print(all_split_questions_collection_name)\n",
    "    r = get_multivector_retriever(persistent_client, embedding_model_name, all_split_questions_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_split_questions_docs, id_key=\"parent_doc_id\")\n",
    "    x.append(r)\n",
    "    print(all_summary_collection_name)\n",
    "    r = get_multivector_retriever(persistent_client, embedding_model_name, all_summary_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_summaries_docs, id_key=\"parent_doc_id\")\n",
    "    x.append(r)\n",
    "\n",
    "    # if all_collection_name in COLLECTIONS:\n",
    "    #     _ = get_multivector_retriever(persistent_client, embedding_model_name, all_collection_name, DATA_PATH, parent_docs=parent_docs, parent_docs_ids=parent_docs_ids, child_docs=all_generated_docs, id_key=\"parent_doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ﺑﻴﺎن أرﺑﺎح اﻟﻨﺘﺎﺋﺞ اﻟﻤﺎﻟﻴﺔ اﻟﺴﻨﻮﻳﺔ ﻟﻠﻌﺎم م2022\\n26 ﻓﺒﺮاﻳﺮ م2023\\nو ﻋ ﻠﻖ اﻟﻤﻬﻨﺪس ﺧﺎﻟﺪ ﺑﻦ ﻋﺒﺪﷲ اﻟﺤﺼﺎن اﻟﺮﺋﻴﺲ اﻟﺘﻨﻔﻴﺬي ﻟﻤﺠﻤﻮﻋﺔ ﺗﺪاول اﻟﺴﻌﻮدﻳﺔ: \"ﺧﻼل اﻟﻌﺎم م2022 ﻋﻤﻠﺖ\\nاﻟﻤﺠﻤﻮﻋﺔ ﻋﻠﻰ ﺗﻘﺪﻳﻢ ﻋﺪد ﻣﻦ اﻟﺘﺤﺴﻴﻨﺎت ﻋﻠﻰ اﻟﺒﻨﻴﺔ اﻟﺘﺤﺘﻴﺔ ﻟﻠﺴﻮق اﻟﻤﺎﻟﻴﺔ اﻟﺴﻌﻮدﻳﺔ ﺑﻬﺪف اﺳﺘﻀﺎﻓﺔ ﻣﺠﻤﻮﻋﺔ\\nﻣﺘﻨﻮﻋﺔ ﻣﻦ اﻟﻤﺼﺪرﻳﻦ واﻟﻤﺴﺘﺜﻤﺮﻳﻦ . وﻳﻌ ﺪ ذﻟﻚ دﻟﻴ ﻼ ﻋﻠﻰ ﻧﺸﺎط اﻹدراﺟﺎت اﻟﻘﻮي اﻟﺬي ﺳﺎﻫﻢ ﻓﻲ ﺗﺮﺳﻴﺦ ﻣﻜﺎﻧﺔ اﻟﺴﻮق\\nاﻟﻤﺎﻟﻴﺔ اﻟﺴﻌﻮدﻳﺔ ﺿﻤﻦ أﺳﻮاق اﻹدراﺟﺎت اﻷﻓﻀﻞ أداء ﻋﻠﻰ ﻣﺴﺘﻮى اﻟﻌﺎﻟﻢ . واﺳﺘﺜﻤﺮﻧﺎ أﻳﻀﺎ ﻓﻲ ﺗﻄﻮﻳﺮ ﺑﻨﻴﺘﻨﺎ اﻟﺘﺤﺘﻴﺔ\\nوﺧﺪﻣﺎﺗﻨﺎ ﻓﻲ إﻃﺎر ﺳﻌﻴﻨﺎ اﻟﻤﺴﺘﻤﺮ ﻟﺘﺤﻘﻴﻖ أﻫﺪاﻓﻨﺎ اﻻﺳﺘﺮاﺗﻴﺠﻴﺔ\".\\nوأﺿﺎف اﻟﺤﺼﺎن: \"ﺗﻤﺎﺷﻴﺎ ﻣﻊ ﺟﻬﻮدﻧﺎ اﻟﻤﺒﺬوﻟﺔ ﻟﺘﻌﺰﻳﺰ اﻟﺒﻨﻴﺔ اﻟﺘﺤﺘﻴﺔ ﻟﺨﺪﻣﺎت اﻟﺘﺪاول وﻣﺎ ﺑﻌﺪ اﻟﺘﺪاول وﺗﺸﺠﻴﻊ ﻋﻤﻠﻴﺎت\\nاﻹدرا ج ﺷﻬﺪﻧﺎ أول ﻋﻤﻠﻴﺔ إدراج ﻣﺰدوج وﻣﺘﺰاﻣﻦ ﺑﻴﻦ ﺗﺪاول اﻟﺴﻌﻮدﻳﺔ وﺳﻮق أﺑﻮﻇﺒﻲ ﻟﻸوراق اﻟﻤﺎﻟﻴﺔ واﻟﺬي ﻳﻤﺜﻞ ﻣﺮﺣﻠﺔ\\nﺟﺪﻳﺪة ﻣﻦ اﻟﺘﻌﺎون ﺑﻴﻦ اﻟﺴﻮق اﻟﻤﺎﻟﻴﺔ اﻟﺴﻌﻮدﻳﺔ واﻷﺳﻮاق اﻟﻤﺎﻟﻴﺔ اﻟﺨﻠﻴﺠﻴﺔ واﻟﺪوﻟﻴﺔ . وﻋﻠﻰ ﻣﺪار اﻟﻌﺎم واﺻﻠﻨﺎ اﻟﺘﻌﺎون\\nﻣﻊ اﻷﺳﻮاق اﻟﻤﺎﻟﻴﺔ اﻹﻗﻠﻴﻤﻴﺔ واﻟﺪوﻟﻴﺔ ﻟﺘﻤﻬﻴﺪ اﻟﻄﺮﻳﻖ ﻟﻠﻤﺰﻳﺪ ﻣﻦ ﻋﻤﻠﻴﺎت اﻹدراج اﻟﻤﺰدوج ﻓﻲ اﻟﻤﺴﺘﻘﺒﻞ اﻟﻘﺮﻳ ﺐ\".\\nواﺧﺘﺘﻢ ﻗﺎﺋ ﻼ: \"ﺧﻼل اﻟﺮﺑﻊ اﻟﺮاﺑﻊ ﻣﻦ اﻟﻌﺎم 2022 م أﻋﻠﻨﺎ ﻋﻦ ﺗﻮﻗﻴﻊ ﺷﺮﻛﺔ \"واﻣ ﺾ\" إﺣﺪى اﻟﺸﺮﻛﺎت اﻟﺘﺎﺑﻌﺔ ﻟﻤﺠﻤﻮﻋﺔ\\nﺗﺪاول اﻟﺴﻌﻮدﻳﺔ ﻋﻦ اﺗﻔﺎﻗﻴﺔ ﺑﻴﻊ وﺷﺮاء ﻣﻊ ﺷﺮﻛﺔ \"اﻟﻤﺒﺎدرات اﻟﺜﺎﻧﻴﺔ ﻟﻼﺳﺘﺜﻤﺎ ر\" ﻟﻼﺳﺘﺤﻮاذ ﻋﻠﻰ %51 ﻣﻦ رأس اﻟﻤﺎل', metadata={'source': 'Press Release - 2022 Results (Stock Market)', 'file_path': '..\\\\..\\\\data\\\\raw\\\\Press Release - 2022 Results (Stock Market).pdf'}),\n",
       " Document(page_content='ﺑﻴﺎن أرﺑﺎح اﻟﻨﺘﺎﺋﺞ اﻟﻤﺎﻟﻴﺔ اﻟﺴﻨﻮﻳﺔ ﻟﻠﻌﺎم م2022\\n26 ﻓﺒﺮاﻳﺮ م2023\\nﻧﺒﺬة ﻋﻦ ﻣﺠﻤﻮﻋﺔ ﺗﺪاول اﻟﺴﻌﻮدﻳﺔ اﻟﻘﺎﺑﻀﺔ\\nﻣﺠﻤﻮﻋﺔ ﺗﺪاول اﻟﺴﻌﻮدﻳﺔ اﻟﻘﺎﺑﻀﺔ اﻟﻤﺠﻤﻮﻋﺔ اﻟﺮاﺋﺪة ﻓﻲ ﻣﺠﺎل اﻷﺳﻮاق اﻟﻤﺎﻟﻴﺔ ﻓﻲ ﻣﻨﻄﻘﺔ اﻟﺸﺮق اﻷوﺳﻂ وﺷﻤﺎل\\nأﻓﺮﻳﻘﻴﺎ ﻫﻲ ﺷﺮﻛﺔ ﻗﺎﺑﻀﺔ ﺗﻀﻢ ﻓﻲ ﻣﺤﻔﻈﺘﻬﺎ 4 ﺷﺮﻛﺎت ﻣﻤﻠﻮﻛﺔ ﻟﻬﺎ ﺑﺎﻟﻜﺎﻣﻞ: ﺷﺮﻛﺔ ﺗﺪاول اﻟﺴﻌﻮدﻳﺔ إﺣﺪى أﻛﺒﺮ\\nاﻷﺳﻮاق اﻟﻤﺎﻟﻴﺔ ﻓﻲ اﻟﻌﺎﻟﻢ ﻣﻦ ﺣﻴﺚ اﻟﻘﻴﻤﺔ اﻟﺴﻮﻗﻴﺔ وﺷﺮﻛﺔ ﻣﺮﻛﺰ إﻳﺪاع اﻷوراق اﻟﻤﺎﻟﻴﺔ )إﻳﺪاع( وﺷﺮﻛﺔ ﻣﺮﻛﺰ ﻣﻘﺎﺻﺔ\\nاﻷوراق اﻟﻤﺎﻟﻴﺔ )اﻟﻤﻘﺎﺻﺔ( وﺷﺮﻛﺔ ﺗﺪاول ﻟﻠﺤﻠﻮل اﻟﻤﺘﻘﺪﻣﺔ )واﻣﺾ( اﻟﻤﺘﺨﺼﺼﺔ ﻓﻲ ﺗﻮﻓﻴﺮ اﻟﺤﻠﻮل اﻟﺘﻘﻨﻴﺔ اﻟﻤﺒﺘﻜﺮة. ﻛﻤﺎ\\nﺗﻤﺘﻠﻚ ﻣﺠﻤﻮﻋﺔ ﺗﺪاول اﻟﺴﻌﻮدﻳﺔ ﺣﺼﺔ ﻗﺪرﻫﺎ %33,12 ﻣﻦ رأس ﻣﺎل ﺷﺮﻛﺔ ﺗﺪاول اﻟﻌﻘﺎرﻳﺔ اﻟﺘﻲ ﺗﻌﻤﻞ ﻓﻲ ﻣﺠﺎل إدارة\\nوﺗﻄﻮﻳﺮ اﻟﻌﻘﺎرات  ﺑﺎﻹﺿﺎﻓﺔ إﻟﻰ ﻣﻠﻜﻴﺘﻬﺎ ﺑﻨﺴﺒﺔ %20 ﻣﻦ ﺷﺮﻛﺔ ﺳﻮق اﻟﻜﺮﺑﻮن اﻟﻄﻮﻋﻲ اﻻﻗﻠﻴﻤﻴﺔ واﻟﺘﻲ ﺗﻬﺪف إﻟﻰ دﻋﻢ\\nاﻟﺸﺮﻛﺎت واﻟﻘﻄﺎﻋﺎت ﻓﻲ اﻟﻤﻨﻄﻘﺔ ﻟﺘﻤﻜﻴﻨﻬﺎ ﻣﻦ اﻟﻮﺻﻮل إﻟﻰ اﻟﺤﻴﺎد اﻟﺼﻔﺮي ﺑﺎﻹﺿﺎﻓﺔ إﻟﻰ ﺿﻤﺎن ﺷﺮاء أرﺻﺪة اﻟﻜﺮﺑﻮن\\nﻟﺘﺨﻔﻴﺾ اﻻﻧﺒﻌﺎﺛﺎت اﻟﻜﺮﺑﻮﻧﻴﺔ ﻓﻲ ﺳﻼﺳﻞ اﻟﻘﻴﻤﺔ.\\nﺗﺘﺒﻨﻰ اﻟﻤﺠﻤﻮﻋﺔ ﻧﻤﻮذج أﻋﻤﺎل ﻣﺘﻨﻮع وﻣﻜﻤﻞ ﻟﺒﻌﻀﻪ ﺑﺎﻋﺘﺒﺎره ﻳﻐﻄﻲ ﺟﻤﻴﻊ اﻟﺨﺪﻣﺎت ذات اﻟﻌﻼﻗﺔ ﻣﻤﺎ ﻳﺴﻤﺢ ﻟﻬﺎ ﺑﺘﻘﺪﻳﻢ\\nﻣﺠﻤﻮﻋﺔ ﻛﺎﻣﻠﺔ ﻣﻦ اﻟﻤﻨﺘﺠﺎت واﻟﺨﺪﻣﺎت اﻟﻤﺮﺗﺒﻄﺔ ﺑﺎﻟﺴﻮق اﻟﻤﺎﻟﻴﺔ. ﺗﺘﻤﺘﻊ اﻟﻤﺠﻤﻮﻋﺔ ﺑﻤﻜﺎﻧﺔ اﺳﺘﺮاﺗﻴﺠﻴﺔ وﺗﻨﺎﻓﺴﻴﺔ ﻗﻮﻳﺔ\\nﻣﺪﻋﻮﻣﺔ ﺑﺤﺠﻤﻬﺎ وإﻣﻜﺎﻧﺎت ﻧﻤﻮﻫﺎ وﻛﻮﻧﻬﺎ ﻋﻨﺼﺮ أﺳﺎﺳﻲ ﻓﻲ اﻟﺘﺤﻮل اﻻﻗﺘﺼﺎدي واﺳﻊ اﻟﻨﻄﺎق. وﺗﺆدي اﺳﺘﻘﻼﻟﻴﺔ أﻋﻤﺎل', metadata={'source': 'Press Release - 2022 Results (Stock Market)', 'file_path': '..\\\\..\\\\data\\\\raw\\\\Press Release - 2022 Results (Stock Market).pdf'})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].invoke('sdfds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PQ_SPLIT_ALL_text_embedding_3_small', 'PS_ALL_text_embedding_3_small']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLLECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MultiVectorRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002A7288030E0>, byte_store=<langchain.storage.file_system.LocalFileStore object at 0x000002A722566690>, docstore=<langchain.storage.encoder_backed.EncoderBackedStore object at 0x000002A7282DD610>, id_key='parent_doc_id')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mما هو\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "x[1].invoke(\"ما هو\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    " _ids = [d.metadata['parent_doc_id'] for d in all_questions_docs]\n",
    "print(len(set(parent_docs_ids).intersection(_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    " _ids = [d.metadata['parent_doc_id'] for d in all_summaries_docs]\n",
    "print(len(set(parent_docs_ids).intersection(_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_docs_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saudi-rag-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
