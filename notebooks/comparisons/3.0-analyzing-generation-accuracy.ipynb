{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.indexing import get_multivector_retriever, get_parent_child_splits\n",
    "from src.generation import QA_SYSTEM_PROMPT, QA_PROMPT, LLAMA_PROMPT_TEMPLATE, MIXTRAL_PROMPT_TEMPLATE\n",
    "from src.generation import get_model, format_docs, get_rag_chain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from src.ingestion import load_pdf\n",
    "\n",
    "import os\n",
    "import chromadb\n",
    "import uuid\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_7532\\1562872242.py:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  DATA_PATH = 'D:\\Ahmed\\saudi-rag-project\\data'\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'D:\\Ahmed\\saudi-rag-project\\data'\n",
    "RAW_DOCS_PATH = os.path.join(DATA_PATH, \"raw\")\n",
    "CHROMA_PATH = os.path.join(DATA_PATH, \"chroma\")\n",
    "INTERIM_DATA_PATH = os.path.join(DATA_PATH, \"interim\")\n",
    "\n",
    "EMBEDDING_MODEL_NAMES = [\n",
    "    \"intfloat/multilingual-e5-small\", \n",
    "    \"intfloat/multilingual-e5-base\", \n",
    "    \"text-embedding-3-small\", \n",
    "    \"text-embedding-3-large\",\n",
    "    \"text-embedding-ada-002\"\n",
    " ]\n",
    "MODEL_NAMES = [\"meta-llama/Llama-3-8b-chat-hf\", \"meta-llama/Llama-3-70b-chat-hf\", \"mistralai/Mixtral-8x22B-Instruct-v0.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievers_results = pd.read_csv('retrieval_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the top collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievers_results['configuration'] = retrievers_results.apply(lambda x: x['collection_name'] + '-' + 'K_' + str(x['k']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_collection_names = retrievers_results.sort_values('recall', ascending=False).head(5).configuration.tolist()\n",
    "top_collection_names += retrievers_results.sort_values('precision', ascending=False).head(5).configuration.tolist()\n",
    "top_collection_names += retrievers_results.sort_values('average_precision', ascending=False).head(5).configuration.tolist()\n",
    "top_collection_names = set(top_collection_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PQS_ALL_text_embedding_3_small-K_11',\n",
       " 'PQS_ALL_text_embedding_3_small-K_13',\n",
       " 'PQS_ALL_text_embedding_3_small-K_3',\n",
       " 'PQS_ALL_text_embedding_3_small-K_5',\n",
       " 'PQS_ALL_text_embedding_3_small-K_9',\n",
       " 'PQ_COMB_Llama_3_70b_chat_hf_intfloat_multilingual_e5_base-K_17',\n",
       " 'PQ_COMB_Llama_3_70b_chat_hf_intfloat_multilingual_e5_base-K_19',\n",
       " 'PQ_COMB_Llama_3_70b_chat_hf_intfloat_multilingual_e5_base-K_3',\n",
       " 'PQ_COMB_Llama_3_70b_chat_hf_text_embedding_3_large-K_13',\n",
       " 'PQ_COMB_Llama_3_70b_chat_hf_text_embedding_3_large-K_15',\n",
       " 'PQ_SPLIT_ALL_text_embedding_3_small-K_11',\n",
       " 'PQ_SPLIT_ALL_text_embedding_3_small-K_3',\n",
       " 'PQ_SPLIT_ALL_text_embedding_3_small-K_5'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_collection_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list of configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = []\n",
    "\n",
    "for collection in top_collection_names:\n",
    "\n",
    "    config_dict = dict()\n",
    "    config_dict['config_name'] = collection\n",
    "    config_dict['collection_name'] = collection.split('-')[0]\n",
    "    config_dict['k'] = collection.split('-')[-1].split('_')[-1]\n",
    "\n",
    "    if 'text_embedding_3_large' in collection:\n",
    "        config_dict['embedding_model_name'] = 'text-embedding-3-large'\n",
    "\n",
    "    elif 'text_embedding_3_small' in collection:\n",
    "        config_dict['embedding_model_name'] = 'text-embedding-3-small'\n",
    "\n",
    "    elif 'text_embedding_ada_002' in collection:\n",
    "        config_dict['embedding_model_name'] = 'text-embedding-ada-002'\n",
    "\n",
    "    elif 'multilingual_e5_small' in collection:\n",
    "        config_dict['embedding_model_name'] = 'intfloat/multilingual-e5-small'\n",
    "\n",
    "    elif 'multilingual_e5_base' in collection:\n",
    "        config_dict['embedding_model_name'] = 'intfloat/multilingual-e5-base'\n",
    "\n",
    "    configs.append(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = pd.read_csv(\"../../data/benchmark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_numbers(text):\n",
    "    text = re.sub(',', '', text)\n",
    "\n",
    "    # This pattern matches both integers and decimal numbers\n",
    "    pattern = r'\\b\\d+\\.?\\d*\\b'\n",
    "\n",
    "    # Find all matches in the text and return them as a list of floats or integers\n",
    "    numbers = re.findall(pattern, text)\n",
    "\n",
    "    # Convert the extracted number strings to appropriate float or int types\n",
    "    return [float(num) if '.' in num else int(num) for num in numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_config(qa_chain, benchmark, config_name, model_name):\n",
    "\n",
    "    benchmark = benchmark.copy()\n",
    "    questions = benchmark.question.tolist()\n",
    "    answers = benchmark.answer.tolist()\n",
    "\n",
    "    generated_answers = qa_chain.batch(questions)\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    for answer, generated_answer in zip(answers, generated_answers):\n",
    "        if set(extract_numbers(answer)).intersection(extract_numbers(generated_answer)):\n",
    "            hits.append(1)\n",
    "        else:\n",
    "            hits.append(0)\n",
    "\n",
    "    benchmark['generated_answer'] = generated_answers\n",
    "    benchmark['correct'] = hits\n",
    "    benchmark['config'] = config_name\n",
    "    benchmark['model'] = model_name\n",
    "\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_model(MODEL_NAMES[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = configs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = get_multivector_retriever(persistent_client, config_dict['embedding_model_name'], config_dict['collection_name'], DATA_PATH, k=config_dict['k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "qa_prompt_template = PromptTemplate.from_template(LLAMA_PROMPT_TEMPLATE.format(system_prompt=QA_SYSTEM_PROMPT, user_message=QA_PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = get_rag_chain(llm, retriever, format_docs, qa_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 33.8 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# model_benchmark = evaluate_model_config(qa_chain, benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_benchmark.correct.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_benchmarks = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [04:14<13:59, 83.94s/it]"
     ]
    }
   ],
   "source": [
    "model_name = MODEL_NAMES[0]\n",
    "llm = get_model(model_name)\n",
    "if 'mistral' in model_name:\n",
    "    qa_prompt_template = PromptTemplate.from_template(MIXTRAL_PROMPT_TEMPLATE.format(system_prompt=QA_SYSTEM_PROMPT, user_message=QA_PROMPT))\n",
    "else:\n",
    "    qa_prompt_template = PromptTemplate.from_template(LLAMA_PROMPT_TEMPLATE.format(system_prompt=QA_SYSTEM_PROMPT, user_message=QA_PROMPT))\n",
    "\n",
    "\n",
    "for config_dict in tqdm(configs):\n",
    "    retriever = get_multivector_retriever(persistent_client, config_dict['embedding_model_name'], config_dict['collection_name'], DATA_PATH, k=config_dict['k'])\n",
    "    qa_chain = get_rag_chain(llm, retriever, format_docs, qa_prompt_template)\n",
    "    model_benchmark = evaluate_model_config(qa_chain, benchmark, config_dict['config_name'], model_name)\n",
    "\n",
    "    model_benchmarks[model_name].append(model_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL_NAMES[1]\n",
    "llm = get_model(model_name)\n",
    "if 'mistral' in model_name:\n",
    "    qa_prompt_template = PromptTemplate.from_template(MIXTRAL_PROMPT_TEMPLATE.format(system_prompt=QA_SYSTEM_PROMPT, user_message=QA_PROMPT))\n",
    "else:\n",
    "    qa_prompt_template = PromptTemplate.from_template(LLAMA_PROMPT_TEMPLATE.format(system_prompt=QA_SYSTEM_PROMPT, user_message=QA_PROMPT))\n",
    "\n",
    "\n",
    "for config_dict in tqdm(configs):\n",
    "    retriever = get_multivector_retriever(persistent_client, config_dict['embedding_model_name'], config_dict['collection_name'], DATA_PATH, k=config_dict['k'])\n",
    "    qa_chain = get_rag_chain(llm, retriever, format_docs, qa_prompt_template)\n",
    "    model_benchmark = evaluate_model_config(qa_chain, benchmark, config_dict['config_name'], model_name)\n",
    "\n",
    "    model_benchmarks[model_name].append(model_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL_NAMES[2]\n",
    "llm = get_model(model_name)\n",
    "if 'mistral' in model_name:\n",
    "    qa_prompt_template = PromptTemplate.from_template(MIXTRAL_PROMPT_TEMPLATE.format(system_prompt=QA_SYSTEM_PROMPT, user_message=QA_PROMPT))\n",
    "else:\n",
    "    qa_prompt_template = PromptTemplate.from_template(LLAMA_PROMPT_TEMPLATE.format(system_prompt=QA_SYSTEM_PROMPT, user_message=QA_PROMPT))\n",
    "\n",
    "\n",
    "for config_dict in tqdm(configs):\n",
    "    retriever = get_multivector_retriever(persistent_client, config_dict['embedding_model_name'], config_dict['collection_name'], DATA_PATH, k=config_dict['k'])\n",
    "    qa_chain = get_rag_chain(llm, retriever, format_docs, qa_prompt_template)\n",
    "    model_benchmark = evaluate_model_config(qa_chain, benchmark, config_dict['config_name'], model_name)\n",
    "\n",
    "    model_benchmarks[model_name].append(model_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saudi-rag-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
